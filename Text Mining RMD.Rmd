---
title: "Text Mining in R"
author: Kaming Yip
output: github_document
---

<font size = "4">
In this file, we will introduce tidy text mining and mainly focus on sentiment analysis with ***Unigram*** and ***Bigram*** in text mining.

## 1. Tidy Text Format
**Token:** meaningful unit of text, most often a word (***Unigram***) but can also be several words (***n-gram***), sentence, or paragraph, that we are interested in using for analysis.

**Tokenization:** the process of splitting text into tokens.

**Tidy Text Format:** a table with one-token-per-row. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix.

Other data structures:\
a) *String*: character vectors\
b) *Corpus*: contain raw strings annotated with additional metadata and details\
c) *Document-Term Matrix*: a collection (i.e. a *corpus*) of documents with one row for each document and one column for each term.\

### 1.1 Tokenization
We first create a typical charater vector (*string*) as our raw text.

```{r 1.1a}
raw_text <- c("This is a video for AD699 Data Mining.",
              "We will mainly focus on tidy text mining -",
              "in this video.",
              "Hope you will like it :)")
raw_text
```

In order to turn it into a tidy text database, we first need to put it into a data frame. Notice that the data frame has printed out as a *tibble*, which is a modern class of data frame within R that has a convenient print method, will not convert strings to factors, and does not use row names.

```{r 1.1b}
# install.packages("dplyr")
library(dplyr)
text_df <- tibble(line = 1:length(raw_text), text = raw_text)
text_df
```

Next step, we need to conduct a process called ***tokenization***, which breaks the text into individual tokens and transform it to a tidy data structure.

```{r 1.1c}
# install.packages("tidytext")
library(tidytext)
text_df %>% unnest_tokens(output = tokens, input = text)
```

The two basic arguments to `unnest_tokens` used here are column names. First we have the output column name that will be created as the text is unnested into it (`tokens`, in this case), and then the input column that the text comes from (`text`, in this case).

Notice that:\
- Punctuation has been stripped (`.`, `-`, `:`, `)`, in this case).\
- By default, `unnest_tokens` converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the `to_lower = FALSE` argument to turn off this behavior).\

### 1.2 Tidying Examples
In this section, we will use [gutenbergr](https://github.com/ropensci/gutenbergr) package as the source of our raw tidy text. This package contains metadata for all Project Gutenberg works as R datasets, so that you can search and filter for particular works before downloading. The dataset `gutenberg_metadata` contains information about each work, pairing Gutenberg ID with title, author, language, etc:

```{r 1.2a}
# install.packages("gutenbergr")
library(gutenbergr)
gutenberg_metadata
```

We can select a public domain work from the Project Gutenberg collection and download it. Here we have NO.11 work as the well-known fairy tale *Alice's Adventures in Wonderland* by Lewis Carroll.

```{r 1.2b}
raw_text <- gutenberg_download(11)
raw_text
tidy_text <- raw_text %>% 
  unnest_tokens(output = tokens, input = text) %>% 
  count(tokens, sort = TRUE)
tidy_text
```

Notice that we have tokenized the text format and unnest it to one-token-per-row structure. However, the most frequent tokens are words that are not useful for an analysis, tipically extremely common words such as "the", "and", "to", "a", and so forth in English. We call them ***stop words***. Often in text analysis, we will want to clean up the stop words. In this case, we will use the `stop_words` dataset in the tidytext package, which contains stop words from three lexicons.

```{r 1.2c}
tidy_text <- raw_text %>% 
  unnest_tokens(output = tokens, input = text) %>% 
  anti_join(stop_words, by = c("tokens" = "word")) %>% 
  count(tokens, sort = TRUE)
tidy_text

# install.packages("ggplot2")
library(ggplot2)
word_freq <- tidy_text %>% 
  head(10)
ggplot(word_freq, aes(x = reorder(tokens, n), y = n)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = n), hjust = 0) +
  ggtitle("The Top 10 Frequent Words in\nAlice's Adventures in Wonderland") +
  theme(plot.title = element_text(hjust = 0.5)) +
  xlab(NULL) +
  ylab("Frequency") +
  coord_flip()
```

## 2. Sentiment Analysis with Tidy Data
When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is *positive* or *negative*, or perhaps characterized by some other more nuanced emotion like *surprise* or *anger*. In text mining, sentiment analysis is esentially important, to understand the information the text really wants to transmit.

One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This isn't the only way to approach sentiment analysis, but it is an often-used approach.

### 2.1 The `sentiments` Dataset
Three general-purpose lexicons for evaluating the opinion or emotion in text are\
* `AFINN`\
* `bing`\
* `nrc`\

#### a. `AFINN`
The `AFINN` lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

```{r 2.1a}
get_sentiments(lexicon = "afinn")
```

#### b. `bing`
The `bing` lexicon categorizes words in a binary fashion into positive and negative categories.

```{r 2.1b}
get_sentiments(lexicon = "bing")
```

#### c. `nrc`
The `nrc` lexicon categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

```{r 2.1c}
get_sentiments(lexicon = "nrc")
```

Not every English word is in the lexicons because many English words are pretty neutral. It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in "no good" or "not true"; a lexicon-based method like this is based on ***Unigrams*** only. We will discuss how to conduct sentiment analysis on ***Bigrams*** later.

### 2.2 Sentiment Analysis with Inner Join
Much as removing stop words is an anti-join operation, performing sentiment analysis is an inner join operation. Let's look at the words with a joy score from the NRC lexicon and find out what the most common joy words are in *Alice's Adventures in Wonderland*.

```{r 2.2a}
raw_text <- gutenberg_download(11)
raw_text
tidy_text <- raw_text %>% 
  unnest_tokens(output = tokens, input = text)
tidy_text
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")
tidy_text %>% 
  inner_join(nrc_joy, by = c("tokens" = "word")) %>% 
  count(tokens, sort = TRUE)

```

We see mostly positive, happy words about garden, baby, and dance here. We also see some words that may not be used joyfully in this book ("found", "white"). One way to deal with these undesired, or misclassified, words is to create a custom lexicon based on the existing ones. In this case, we use `nrc` lexicon.

```{r 2.2b}
custom_nrc <- get_sentiments("nrc")
custom_nrc[custom_nrc$word == "found", ]
custom_nrc$sentiment[custom_nrc$word == "found"] <- "neutral"
custom_nrc[custom_nrc$word == "found", ]
```

Now we want to examine how sentiment is throughout the entire book. This time, we will try `bing` lexicon instead of `nrc`.

```{r 2.2c}
library(tidyr)
alice_sentiment <- raw_text %>% 
  mutate(linenumber = row_number()) %>% 
  unnest_tokens(tokens, text) %>% 
  inner_join(get_sentiments("bing"), by = c("tokens" = "word")) %>% 
  count(index = linenumber %/% 25, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative,
         fill = ifelse(sentiment >= 0, "positive score", "negative score"))
alice_sentiment
ggplot(alice_sentiment, aes(x = index, y = sentiment, fill = factor(fill))) +
  geom_col(show.legend = FALSE)
```

We can see in the figure how the plot of the book changes toward a slightly negative sentiment over the trajectory of the story. This is a interesting finding in this case.

### 2.3 Wordclouds
Wordclouds can be applied in text mining to visualize the word frequency.
```{r 2.3a}
library(wordcloud)
raw_text <- gutenberg_download(11)
tidy_text <- raw_text %>% 
  unnest_tokens(output = tokens, input = text) %>% 
  anti_join(stop_words, by = c("tokens" = "word")) %>% 
  count(tokens, sort = TRUE)
ggplot(head(tidy_text, 10), aes(x = reorder(tokens, n), y = n)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = n), hjust = 0) +
  ggtitle("The Top 10 Frequent Words in\nAlice's Adventures in Wonderland") +
  theme(plot.title = element_text(hjust = 0.5)) +
  xlab(NULL) +
  ylab("Frequency") +
  coord_flip()

tidy_text %>% 
  with(wordcloud(tokens, n, max.words = 100))
```

As we can see from both the bar plot and the wordcloud, the word *alice* is the most frequent word, and *time*, *queen*, *king*, *turtle*, and *mock* those are also frequent words in the text. And we can generally get what this fairy tale talks about through the results.

Another application of wordcloud is called `comparison.cloud()`. Let's do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words.

```{r 2.3b}
library(reshape2)
raw_text <- gutenberg_download(11)
tidy_text <- raw_text %>% 
  unnest_tokens(output = tokens, input = text) %>% 
  inner_join(get_sentiments("bing"), by = c("tokens" = "word")) %>% 
  count(tokens, sentiment, sort = TRUE) %>% 
  acast(tokens ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("red", "blue"), max.words = 100)
```

## 3. Relationships between Words: Unigram and Bigram
So far we've considered words as individual units, and considered their relationships to sentiments pr to documents. However, many interesting text analyses are based on the relationships between words. In this part, we will explore some of the methods tidytext offers for calculating and visualizing relationships between words in text dataset.

### 3.1 Tokenizing by Bigram
We've been using the `unnest_tokens` function to tokenize by words, which is useful for the kinds of sentiment and frequency analyses we've been doing so far. But we can also use the function to tokenize into consecutive sequences of words, called **n-grams**. In this video, we will set n to 2, often called **Bigram**.

```{r 3.1a}
library(dplyr)
library(tidytext)
library(gutenbergr)
raw_text <- gutenberg_download(11)
tidy_text <- raw_text %>% 
  unnest_tokens(output = bigram, input = text, token = "ngrams", n = 2)
tidy_text
```

This data structure is still a variation of the tidy text format. It is structured as one-token-per-row, but each token now represents a bigram. Notice that these bigrams overlap: *"alice's adventures"* is one token, while *"adventures in"* is another.

```{r 3.1b}
tidy_text %>% 
  count(bigram, sort = TRUE)
```

As you might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as `said the` and `of the`: what we call "stop-words". This time, in order to remove those cases, we use `separate()` function from `tidyr` package to split a column into two columns, "word 1" and "word 2" and we can remove cases where either is a stop-word.

```{r 3.1c}
library(tidyr)
bigrams_separated <- tidy_text %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
bigrams_separated

bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE)
bigrams_filtered

bigrams_united <- bigrams_filtered %>% 
  unite(bigram, word1, word2, sep = " ")
bigrams_united
```

### 3.2 Using Bigrams to Provide Context in Sentiment Analysis
Our sentiment analysis approach in previous part simply counted the apprearance of positive or negative words, according to a reference lexicon. One of the problems with this approach is that a word's context can matter nearly as much as its presence. For example, the words "happy" and "like" will be counted as positive, even in a sentence like "I'm not **happy** and I don't **like** it!"

Now that we have the data organized into bigrams, it's easy to tell how often words are preceded by a word like "not":

```{r 3.2a}
bigrams_separated %>% 
  filter(word1 == "not") %>% 
  count(word1, word2, sort = TRUE)
```

By performing sentiment analysis on the bigram data, we can examine how often sentiment-associated words are preceded by "not" or other negative words. We could use this to ignore or even reverse their contribution to the sentiment score.

Let’s use the `AFINN` lexicon for sentiment analysis, which you may recall gives a numeric sentiment value for each word, with positive or negative numbers indicating the direction of the sentiment.
```{r 3.2b}
AFINN <- get_sentiments(lexicon = "afinn")
AFINN
reverse_words <- bigrams_separated %>% 
  filter(word1 %in% c("don't", "not", "no", "never")) %>% 
  inner_join(AFINN, by = c(word2 = "word")) %>% 
  count(word2, value, sort = TRUE)
reverse_words
```

For example, the most common sentiment-associated word to follow those nagtive words was "like", which would normally have a positive score of 2.

It's worth asking which words contributed the most in the "wrong" direction. To compute that, we can multiply their value by the number of times they appear.

```{r 3.2c}
library(ggplot2)
reverse_words %>% 
  mutate(contribution = n * value) %>% 
  arrange(desc(abs(contribution))) %>% 
  head(10) %>% 
  mutate(word2 = reorder(word2, contribution)) %>% 
  ggplot(aes(x = word2, y = contribution, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Word preceded by reverse words") +
  ylab("Sentiment value * number of occurrences") +
  coord_flip()
```

The bigram "don't/not/no like" was overwhelmingly the largest causes of misidentification, making the text seem much more positive than it is. But we also can see some phrases suggest text is more negative than it is.

To see the influence of the reverse words toward our context sentiment, we will try to examine how sentiment is throughout the entire book and compare the result when we apply unigram to the sentiment analysis.

```{r 3.2d}
library(tidyr)
alice_bigram_sentiment <- raw_text %>% 
  mutate(linenumber = row_number()) %>% 
  unnest_tokens(output = bigram, input = text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  inner_join(get_sentiments("bing"), by = c("word2" = "word")) %>% 
  mutate(contribution = ifelse(word1 %in% c("don't", "no", "not", "never"),
                               ifelse(sentiment == "negative", "positive", "negative"), sentiment)) %>% 
  count(index = linenumber %/% 25, contribution) %>% 
  spread(contribution, n, fill = 0) %>% 
  mutate(contribution = positive - negative,
         fill = ifelse(contribution >= 0, "positive score", "negative score"))
alice_bigram_sentiment
ggplot(alice_bigram_sentiment, aes(x = index, y = contribution, fill = factor(fill))) +
  geom_col(show.legend = FALSE) +
  ggtitle("The Sentiment Analysis for Bigram")
ggplot(alice_sentiment, aes(x = index, y = sentiment, fill = factor(fill))) +
  geom_col(show.legend = FALSE) +
  ggtitle("The Sentiment Analysis for Unigram")
```

Although it's hard to tell what changes exactly occur in this case, we can tell there is some changes between unigram sentiment analysis and bigram sentiment analysis. In future research, we can apply both analysis process to examine how sentiment really is throughout a given text. 

### 3.3 Create a Bigram Wordcloud

```{r 3.3a}
library(wordcloud)
library(ggplot2)
library(reshape2)
raw_text <- gutenberg_download(11)
tidy_text <- raw_text %>% 
  unnest_tokens(output = bigram, input = text, token = "ngrams", n = 2)
tidy_text
bigrams_united <- tidy_text %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  unite(bigram, word1, word2, sep = " ")
bigrams_united

ggplot(head(bigrams_united, 10), aes(x = reorder(bigram, n), y = n)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = n), hjust = 0) +
  ggtitle("The Top 10 Frequent Bigrams in\nAlice's Adventures in Wonderland") +
  theme(plot.title = element_text(hjust = 0.5)) +
  xlab(NULL) +
  ylab("Frequency") +
  coord_flip()

bigrams_united %>% 
  with(wordcloud(bigram, n, max.words = 100))

bigram_compare_wordcloud <- tidy_text %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  inner_join(get_sentiments("bing"), by = c("word2" = "word")) %>% 
  mutate(contribution = ifelse(word1 %in% c("don't", "no", "not", "never"),
                               ifelse(sentiment == "negative", "positive", "negative"), sentiment)) %>% 
  unite(bigram, word1, word2, sep = " ") %>% 
  count(bigram, contribution, sort = TRUE) %>% 
  acast(bigram ~ contribution, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("red", "blue"), max.words = 50)
```



--------------
This content was mainly built from material contained in "Text Mining in R" and "Data Mining for Business Analytics". So if you want to dig into these topics more, we suggest you take a look at those rescources.